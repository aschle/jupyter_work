{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Second part: Machine learning in Spark\n",
    "## Task 1 - Toxicology prediction with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Spark program that builds a toxicology prediction model form the  toxicity.txt dataset, using Support Vector Machines (SVM). In order to evaluate the model, use 80% of the dataset as training set, and save the remaining 20% for testing. As evaluation metric you can use the area under the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data and preparation\n",
    "Looking at the file in LIBSVM format, each line looks as follows:\n",
    "\n",
    "    1.0 855:1.0 15924:1.0 51912:1.0 54501:2.0 57936:1.0 74579:2.0 76053:1.0 81127:4.0 ...\n",
    "    <label> <index>:<value> <index>:<value> ...\n",
    " \n",
    "Here the **label** indicates the molecular toxicity, 1 means toxic and 0 means non-toxic. The **features** are molecular descriptors (CDK molecular signatures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.5681818181818181\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load dataset in LIBSVM format.\n",
    "val data = MLUtils.loadLibSVMFile(sc, \"toxicity.txt\")\n",
    "\n",
    "// Split data into training (80%) and test (20%).\n",
    "val splits = data.randomSplit(Array(0.8, 0.2), seed = 11L)\n",
    "val training = splits(0).cache() // caching training data\n",
    "val test = splits(1)\n",
    "\n",
    "// Run training algorithm to build the model\n",
    "// SVM using SGD (Stochastic Gradient Descent) optimization method\n",
    "val numIterations = 100\n",
    "val model = SVMWithSGD.train(training, numIterations)\n",
    "\n",
    "// Clear the default threshold.\n",
    "model.clearThreshold()\n",
    "\n",
    "// Compute raw scores on the test set.\n",
    "// Compute distance from hyperplane for each test example\n",
    "val scoreAndLabels = test.map { point =>\n",
    "  val score = model.predict(point.features)\n",
    "  (score, point.label)\n",
    "}\n",
    "\n",
    "// Get evaluation metrics.\n",
    "// Compute the area under the ROC curve using the MLlib primitive\n",
    "val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "val auROC = metrics.areaUnderROC()\n",
    "\n",
    "println(\"Area under ROC = \" + auROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the limitation of Hadoop that makes Machine Learning infeasible? How does Spark overcome this limitation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop comes with HDFS and stores large data on disk in a distributed way, it uses MapReduce to do distributed processing on those data. MapReduce jobs are for batch analytics and run sequentially.\n",
    "\n",
    "Spark can stores data in-memory using resilient distributed datasets (RDD). \n",
    "\n",
    "Most machine leraning algorithms run on the same data set iteratively and use data retrieved from iterations in next iterations. In hadoop MapReduce there is no easy way to communicate a shared state from iteration to iteration. Also using the same data over and over again, creates a need for in memory caching, so the trainig data can be cached using Spark. Reading from disk all the time in hadoop would slow things down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you rate the model that you obtained in this task? Is that a good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the **ROC** curve is `0.568`. In the lecture it was said that \"the area under the ROC curve for a good model is close to 1\".\n",
    "\n",
    "The value is close to `0.5` so it is more a random predictor and therefore not a useful model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache_Toree",
   "language": "",
   "name": "apache_toree"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
